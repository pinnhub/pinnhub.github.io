<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Hybrid Regression and Explainable AI for Phase Transition Analysis of two-flavour Quark Matter</title>
  <!-- Google Fonts -->
<!--   <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600&display=swap" rel="stylesheet"> -->
   <style>
    body {
      font-family: 'Times New Roman', Times, serif;
     
      margin: 0;
      padding: 0;
      background-color: #f4f4f4;
      color: #333;
    }
    
    header {
      text-align: center;
      padding: 20px;
      background-color: #fff;
      box-shadow: 0 0 5px rgba(0,0,0,0.1);
    }
    header h1 {
      font-size: 28px;
      color: #2c3e50;
      margin: 0;
      padding: 10px;
    }
    header h2 {
      font-size: 22px;
      color: #34495e;
      margin: 0;
      padding-bottom: 10px;
    }
    .content {
      max-width: 1200px;
      margin: 30px auto;
      background-color: #fff;
      padding: 20px;
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
      text-align: left;
      line-height: 1.6;
    }
    .section {
      margin-bottom: 30px;
    }
    .section h2 {
      font-size: 20px;
      color: #34495e;
      margin-bottom: 10px;
      border-bottom: 2px solid #ddd;
      padding-bottom: 5px;
    }
    .section p, 
    .section li {
      font-size: 18px;
      margin: 10px 0;
    }
    ul {
      padding-left: 20px;
    }
    .figure-container {
      display: flex;
      justify-content: space-around;
      align-items: center;
      margin: 20px 0;
    }
    .figure-container img {
      width: 45%;
      height: auto;
      box-shadow: 0 0 5px rgba(0,0,0,0.1);
    }
    img.box-diagram {
      display: block;
      margin: 30px auto;
      max-width: 80%;
      height: auto;
    }
  </style>
</head>
  <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<body>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <header>
    <h1>Hybrid Regression and Explainable AI for Phase Transition Analysis of two-flavour Quark Matter</h1>
    <h2>Pradipta K. Banerjee, Tamal K. Mukherjee</h2>
  </header>

  <div class="content">
    <!-- Box Diagram Section -->
    <section>
      <h3>Bock Diagram of Proposed Methodology</h3>
      <img class="box-diagram" src="bd.JPG" alt="Bock Diagram of Proposed Methodology">
    </section>

    <!-- Methodology Sections -->
    <section class="section">
      <h2 style="text-align: center;">Training Phase</h2>
       <p>
      Due to data scarcity in QCD studies, we adopt a handcrafted feature extraction strategy, incorporating statistical descriptors like <strong>kurtosis</strong> and <strong>skewness</strong>,
         along with a novel feature, the <strong>Maximum Separation Measure (MSM)</strong>. 
    </p>
      <p>
    Given a signal vector \( \mathbf{x} = [x_1, x_2, \ldots, x_n] \), where each \( x_i \in \mathbb{R} \) represents sequential measurements, the MSM is mathematically defined as:
  </p>

  <p style="text-align: center; font-style: italic;">
    \[
      \text{MSM}(\mathbf{x}) = \max_{i \in \{1, \ldots, n-1\}} \|x_{i+1} - x_i\|^2
    \]
  </p>

  <p>
    This measure effectively quantifies the largest squared distance between consecutive data points, making it highly sensitive to sudden changes indicative of phase boundaries.
  </p>

    <p>
      These features are analyzed using <strong>SHapley Additive exPlanations (SHAP)</strong>, enabling model-agnostic interpretability. 
      The top features, including MSM, kurtosis, and skewness, are selected for unsupervised clustering to distinguish between crossover and first-order transitions,
      facilitating accurate phase transition identification.  <p>
    Figure~<span id="fig:shap_beeswarm_plot">1</span> illustrates the global importance of these top three features across all samples in the binary classification task, confirming their significant contribution toward distinguishing between crossover and first-order phase transitions. These analyses facilitated the selection of the top three features, namely <strong>Kurtosis</strong>, <strong>Skewness</strong>, and <strong>MSM</strong>, for subsequent unsupervised clustering analyses aimed at categorizing the phase transitions.
  </p>
    </p>

    <p>
      We employ a <strong>Gaussian Mixture Model (GMM)</strong> for clustering crossover and first-order phase transitions, leveraging its ability to handle small datasets and unlabeled data. The GMM models the probability distribution as 
      <span>\( p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x} | \mu_k, \Sigma_k) \)</span>, 
      where \( \pi_k \), \( \mu_k \), and \( \Sigma_k \) represent the mixture weights, mean, and covariance of each component.
    </p>

    <p>
      Using the <strong>Expectation-Maximization (EM)</strong> algorithm, we iteratively optimize these parameters, enabling effective soft clustering and probabilistic classification. The GMM is trained with empirically optimized hyperparameters, ensuring stable and reproducible clustering results across the dataset.
    </p>

    <p>
      To delineate the phase boundary, this study employs a <strong>hybrid regression</strong> approach combining parametric and semi-parametric techniques. The <strong>Multi-Task Learning (MTL)</strong> model uses a Multi-Layer Perceptron (MLP) to classify phases and predict transition temperatures for each coupling constant.
    </p>

    <p>
      After classification, a <strong>parametric regression</strong> is performed using an MLP regressor to predict the transition temperatures from \(\sigma\)-\(T\) data. Subsequently, <strong>Kernel Ridge Regression (KRR)</strong><sup>[1]</sup> is applied to accurately establish the phase boundary, where the prediction is given by the following equation:
    </p>

    <p style="text-align: center; font-style: italic;">
      <span>\( f(\mathbf{x}) = \sum_{i=1}^{n} \alpha_i k(\mathbf{x}, \mathbf{x}_i) \)</span>
    </p>

    <p>
      where \(\alpha\) denotes the regression coefficients and \(k(\mathbf{x}, \mathbf{x}_i)\) is the kernel function. This approach enhances the precision and robustness of the phase boundary predictions.
    </p>
      <h3>Step 1: Feature Extraction</h2>
      <p>
        The process begins with a <strong>labeled</strong> dataset containing two types of phase transition data: <em>crossover</em> and <em>first order</em>. From this data, various statistical features are extracted, including:
      </p>
      <p>
  The extracted statistical features include <strong>msm</strong>, a newly proposed feature introduced in the study; 
  <strong>median</strong>, representing the central value of the dataset; 
  <strong>range</strong>, the difference between the maximum and minimum values; 
  <strong>IQR</strong> (Interquartile Range), which is the difference between the 75th and 25th percentiles; 
  <strong>skewness</strong>, measuring the asymmetry in the data distribution; 
  <strong>kurtosis</strong>, indicating the “tailedness” of the distribution; 
  <strong>entropy</strong>, a quantifier of disorder or uncertainty; 
  <strong>first_deriv_mean</strong>, the average value of the first derivative of the key signal; 
  <strong>first_deriv_std</strong>, the standard deviation of the first derivative; 
  <strong>first_deriv_max</strong>, the maximum observed value of the first derivative; and 
  <strong>first_deriv_min</strong>, the minimum observed value of the first derivative.
</p>

      <p>
        These features help in characterizing the system's behavior near the phase transition.
      </p>
    </section>

    <section class="section">
      <h3>Step 2: Feature Importance using Random Forest and SHAP</h2>
      <p>
        In this step, a <strong>Random Forest classifier</strong> is used to rank the importance of the extracted features by differentiating between crossover and first order transitions. To provide interpretability:
      </p>
      <ul>
        <li><strong>SHAP (SHapley Additive exPlanations)</strong> computes contribution scores, offering both local and global insights into which features are influential in making predictions.</li>
        <li>Figures (1) and (2) in the study illustrate the score distribution of the important features as obtained by the explainable AI approach. <p>
        After assessing feature importance via Random Forest and SHAP, the <strong>top-3 most important features</strong> are selected for further analyses. This Reduces the dimensionality of the problem.
      </p></li>
      <!-- Flex container for images -->
          <div class="figure-container">
            <img src="rf.png" alt="Random Forest Feature Importance">
            <img src="shap.png" alt="SHAP Value Distribution" id="fig:shap_beeswarm_plot">
          </div>
      </ul>
    </section>

    

    <section class="section">
      <h3>Step 3: Unsupervised Classification with GMM</h2>
      <p>
        With the selected features, a <strong>Gaussian Mixture Model (GMM)</strong> is trained using the labeled data. The methodology here involves:
      </p>
      <ul>
        <li>Empirically setting the hyperparameters (such as <p>
  The hyperparameters are set as: 
  <span>\(\text{covariance\_type} = \text{'diag'}\)</span>, 
  <span>\(\text{tol} = 10^{-3}\)</span>, 
  <span>\(\text{reg\_covar} = 0.04\)</span>, 
  <span>\(\text{max\_iter} = 500\)</span>, 
  <span>\(\text{n\_init} = 1\)</span>, and 
  <span>\(\text{random\_state} = 45\)</span>.
</p>
) based on the training data.</li>
        <li>Keeping the GMM parameters fixed during the testing phase, allowing for unsupervised classification by computing the probability of new data belonging to either the crossover or first order group.</li>
      </ul>
    </section>

    <section class="section">
      <h3>Step 4: Dual MLP for Confined vs. Deconfined Classification</h2>
      <p>
        After classifying the data into crossover and first order types using the GMM, the methodology further distinguishes between the confined and deconfined phases:
      </p>
      <ul>
        <li>Two separate <strong>Multi-Layer Perceptrons (MLP)</strong> are trained:
          <ul>
            <li>One MLP for classifying confined versus deconfined phase in the crossover data.</li>
            <li>Another MLP for the same classification in the first-order data.</li>
          </ul>
        </li>
        <li>This dual approach ensures each MLP is tailored to the unique characteristics of its corresponding dataset.</li>
      </ul>
    </section>

    <section class="section">
      <h3>Step 5: MTL for Transition Temperature Regression</h2>
          <ul>
        <li>An additional <strong>MLP regressor</strong> is trained to predict the transition temperatures for both crossover and first-order data.</li>
        <li>This parametric regression model learns from all confined and deconfined data across both data.</li>
      </ul>
    </section>

    <section class="section">
      <h3>Step 6: Non-Parametric Regression for Phase Boundary Prediction</h2>
      <p>
        Finally, with the transition temperatures predicted, the phase boundary is determined using various non-parametric regression techniques:
      </p>
      <ul>
        <li><strong>Gaussian Process Regression</strong></li>
        <li><strong>Polynomial Regression</strong></li>
        <li><strong>Kernel Regression</strong></li>
        <li><strong>k-Nearest Neighbors (kNN) Regression</strong></li>
        <li><strong>Kernel Ridge Regression (KRR)</strong></li>
        <li>A <strong>hybrid regression model</strong> combining KRR and kNN.</li>
      </ul>
      <p>
        Among these techniques, <strong>KRR</strong> is found to be most effective on the validation dataset. The <em>soft probability assignments</em> from the GMM further help in defining a probable zone of phase transition
        over the predicted boundary. 
  
      </p>
    </section>

    <section class="section">
      <h2>In brief</h2>
      <p>
        The proposed methodology integrates statistical feature extraction, supervised feature ranking with explainable AI (SHAP), unsupervised classification via GMM, and both parametric and non-parametric regression techniques. The approach ensures classification of phase transitions in two-flavour quark matter,  providing  prediction of the phase boundary
        and probable second order transition zone.
      </p>
    </section>

     <h2 style="text-align: center;">Testing Phase</h2>
   <img class="box-diagram" src="testbd.JPG" alt="Bock Diagram of Proposed Testing Phase">
    <p>
  In the <strong>testing phase</strong>, a new <em>unlabelled dataset</em> is introduced for inference. The first step involves extracting the <strong>top three features</strong>
      from the raw test data using the same feature selection logic employed during the training phase. These features are then passed through the pretrained (with hyperparameters)
      <strong>Gaussian Mixture Model (GMM)</strong>, where the model—configured with fixed hyperparameters—classifies each input sample into one of
      two categories: <strong>crossover</strong> or <strong>first-order</strong> phase transition.
</p>

<p>
  The samples predicted as crossover are then fed into already trained <strong>MLP-1</strong> model, while the first-order samples are fed to
  another trained <strong>MLP-2</strong> model. Each of these multi-layer perceptrons has been trained to detect whether the input data corresponds to a <strong>confined</strong> or
  <strong>deconfined</strong> phase and hence predict in the testing phase.
</p>

<p>
  Once classification into confined and deconfined states is complete for both crossover and first-order samples, 
  these predicted phases are aggregated and passed into a shared <strong>MLP-Regressor</strong>. 
  This regressor, trained using a parametric regression strategy during the training phase, 
  is responsible for estimating the <strong>transition temperatures (TT)</strong>.
  The regression model outputs two transition temperatures: one for crossover and another for first-order transitions.
</p>

<p>
  These predicted transition temperatures serve as input to a non-parametric model, 
  specifically the <strong>Kernel Ridge Regression (KRR)</strong> framework. The KRR model maps the transition temperature data to 
  a continuous surface, allowing it to predict the <strong>phase boundary</strong> across varying thermodynamic parameters. 
  Furthermore, by leveraging the <em>soft probability assignments</em> from the GMM classifier, the method identifies and highlights
  regions on this boundary that are likely to correspond to <strong>second-order phase transitions</strong>.
</p>




<!--      <h2 style="text-align: center;">Few Experimental Results</h2>

    <h3 style="text-align: center;">Comparative Study of Regression Methods</h3>

<table border="1" cellspacing="0" cellpadding="10" style="width: 80%; margin: 20px auto; font-size: 18px; font-family: 'Times New Roman', Times, serif; border-collapse: collapse; background-color: #fff; text-align: left;">
  <thead style="background-color: #e0e0e0;">
    <tr>
      <th>Method</th>
      <th>Mean Squared Error (MSE)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Polynomial Regression</td>
      <td>0.162551</td>
    </tr>
    <tr>
      <td>Kernel Ridge Regression</td>
      <td>0.000004</td>
    </tr>
    <tr>
      <td>KNN Regression</td>
      <td>0.000091</td>
    </tr>
    <tr>
      <td>Hybrid Model</td>
      <td>0.000033</td>
    </tr>
    <tr>
      <td>Gaussian Process Regression</td>
      <td>0.000140</td>
    </tr>
  </tbody>
</table>
 -->
  </div>
  
  
</body>
</html>
